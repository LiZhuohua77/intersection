"""
@file: evaluate.py
@description:
æ—  GUI è¯„ä¼°è„šæœ¬ï¼Œç”¨äºè¯„ä¼°å·²è®­ç»ƒå¥½çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨äº¤é€šåœºæ™¯ä¸­çš„è¡¨ç°ã€‚
æ”¯æŒåŠ è½½ PPO / SAGI-PPO / Lagrangian PPO ç­‰æ¨¡å‹ï¼Œåœ¨ TrafficEnv ä¸­è¿è¡Œå¹¶è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼Œ
ç”Ÿæˆé‡åŒ–æŒ‡æ ‡å¹¶è®°å½•è¯¦ç»†æ—¥å¿—æ•°æ®ï¼ˆCSV å’Œæ›²çº¿å›¾åƒï¼‰ã€‚

ä¸»è¦å‡½æ•°:
- set_seed(seed): è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤æ€§
- parse_args(): è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œæ”¯æŒé…ç½®è¯„ä¼°ç®—æ³•ç±»å‹ã€æ¨¡å‹è·¯å¾„ã€è¯„ä¼°å›åˆæ•°ã€åœºæ™¯ç­‰
- main(): ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€ç¯å¢ƒåˆå§‹åŒ–ã€è¯„ä¼°å¾ªç¯å’Œç»“æœç»Ÿè®¡
"""

import pygame
import torch
import time
import os
import argparse
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt

from config import *
from stable_baselines3 import PPO
from game_engine import GameEngine
from traffic_env import TrafficEnv
from sagi_ppo import SAGIPPO
from ppo_lagrangian import PPOLagrangian


def set_seed(seed: int):
    """
    è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒç»“æœçš„å¯é‡å¤æ€§
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œé…ç½®è¯„ä¼°è¿‡ç¨‹
    """
    parser = argparse.ArgumentParser(
        description="Headless evaluation of a trained PPO/SAGI-PPO/PPOLagrangian agent."
    )
    parser.add_argument(
        "--algo",
        type=str,
        default="sagi_ppo_mlp",
        choices=[
            "sagi_ppo_mlp",
            "sagi_ppo_gru",
            "ppo_gru",
            "ppo_mlp",
            "ppo_lagrangian_gru",
            "ppo_lagrangian_mlp",
        ],
        help="The algorithm of the trained agent to evaluate.",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="D:/Code/intersection/models/expt1/sagi_ppo_mlp_final_model.zip",
        help="Path to the saved model .zip file (e.g., 'models/final_model.zip').",
    )
    parser.add_argument(
        "--num-episodes",
        type=int,
        default=1,
        help="Total number of episodes to run for evaluation.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=496,
        help="Random seed for reproducibility.",
    )
    parser.add_argument(
        "--scenario",
        type=str,
        default="agent_only_simple",
        choices=["agent_only_simple", "random_traffic", "crossing_conflict", "mixed_traffic","left_turn_vs_straight"],
        help="Evaluation scenario name in TrafficEnv/TrafficManager.",
    )

    return parser.parse_args()


def handle_episode_end(
    env,
    info,
    eval_stats: dict,
    episode_reward: float,
    episode_cost: float,
    episode_len: int,
    episode_discounted_cost: float,
    episode_idx: int,
    log_save_dir: str,
    truncated: bool,
):
    """
    ç»Ÿä¸€å¤„ç†ä¸€ä¸ªå›åˆç»“æŸæ—¶çš„ï¼š
    - outcome åˆ¤å®šï¼ˆsuccess / collision / off_track / timeoutï¼‰
    - è½¨è¿¹æ—¥å¿—ä¿å­˜ï¼ˆCSVï¼‰
    - æŒ‡æ ‡è®¡ç®—ï¼ˆavg_jerk / avg|sCTE|ï¼‰
    - eval_stats æ›´æ–°
    - ç»˜å›¾ï¼ˆRL Agent é€Ÿåº¦ã€NPC é€Ÿåº¦ï¼‰ï¼Œå¹¶å…³é—­å›¾é˜²æ­¢å†…å­˜æ³„æ¼
    """

    # --- 1. åˆ¤å®š outcome å¹¶æ›´æ–°åˆ†ç±»ç»Ÿè®¡ ---
    outcome = "success"
    if info.get("failure") == "collision":
        eval_stats["collision"] += 1
        outcome = "collision"
        print(f"ç»“æœ: ç¢°æ’ (Collision) | å›åˆå¥–åŠ±: {episode_reward:.2f}")
    elif info.get("failure") == "off_track":
        eval_stats["off_track"] += 1
        outcome = "off_track"
        print(f"ç»“æœ: åç¦»è½¨è¿¹ (Off-track) | å›åˆå¥–åŠ±: {episode_reward:.2f}")
    elif truncated:
        eval_stats["timeout"] += 1
        outcome = "timeout"
        print(f"ç»“æœ: è¶…æ—¶ (Timeout) | å›åˆå¥–åŠ±: {episode_reward:.2f}")
    else:
        eval_stats["success"] += 1
        print(f"ç»“æœ: æˆåŠŸ (Success) | å›åˆå¥–åŠ±: {episode_reward:.2f}")

    # --- 2. ä¿å­˜è½¨è¿¹æ—¥å¿— CSV ---
    log_df = None
    if "episode_log" in info and info["episode_log"]:
        try:
            log_df = pd.DataFrame(info["episode_log"])
            log_filename = f"ep_{episode_idx}_{outcome}.csv"
            log_filepath = os.path.join(log_save_dir, log_filename)
            log_df.to_csv(log_filepath, index=False)
            print(f"è½¨è¿¹æ—¥å¿—å·²ä¿å­˜åˆ°: {log_filepath}")
        except Exception as e:
            print(f"ä¿å­˜è½¨è¿¹æ—¥å¿—å¤±è´¥: {e}")

    # --- 3. è®¡ç®— jerk / |sCTE| ---
    avg_jerk = 0.0
    avg_scte = 0.0
    signed_mean_scte = 0.0  # å¯é€‰ï¼Œç”¨æ¥çœ‹æœ‰ä¸ç»™åä¸€ä¾§
    episode_energy_net_kwh = 0.0
    episode_energy_consume_kwh = 0.0
    episode_energy_regen_kwh = 0.0

    if log_df is not None and not log_df.empty:
        # jerk
        if "action_accel" in log_df.columns:
            accels = log_df["action_accel"] * MAX_ACCELERATION
            if len(accels) > 1:
                jerks = np.abs(np.diff(accels) / env.dt)
                if len(jerks) > 0:
                    avg_jerk = float(np.mean(jerks))

        # |sCTE|
        if "signed_cross_track_error" in log_df.columns:
            scte_series = log_df["signed_cross_track_error"]
            avg_scte = float(np.mean(np.abs(scte_series)))  # å¹³å‡ç»å¯¹å€¼
            signed_mean_scte = float(scte_series.mean())   # æœ‰ç¬¦å·å‡å€¼ï¼ˆåå·¦/åå³ï¼‰

        if "raw_power" in log_df.columns:
            # P_elec_kWï¼šç¬æ—¶ç”µåŠŸç‡ï¼ˆæ­£ï¼šè€—ç”µï¼Œè´Ÿï¼šå›æ”¶ï¼‰
            power_kw = log_df["raw_power"].to_numpy(dtype=np.float32)

            dt = env.dt  # ç§’
            dt_hour = dt / 3600.0  # å°æ—¶

            # åªç®—æ¶ˆè€—ï¼ˆæ­£åŠŸç‡ï¼‰
            power_pos_kw = np.clip(power_kw, 0, None)
            # åªç®—å›æ”¶ï¼ˆè´ŸåŠŸç‡ï¼‰
            power_neg_kw = np.clip(power_kw, None, 0)

            episode_energy_consume_kwh = float(np.sum(power_pos_kw) * dt_hour)
            # å›æ”¶èƒ½é‡ç”¨æ­£å€¼è¡¨ç¤ºï¼š-è´ŸåŠŸç‡
            episode_energy_regen_kwh = float(-np.sum(power_neg_kw) * dt_hour)
            # å‡€èƒ½é‡ = æ¶ˆè€— - å›æ”¶ï¼ˆå¯ä»¥ä¸ºè´Ÿï¼‰
            episode_energy_net_kwh = episode_energy_consume_kwh - episode_energy_regen_kwh

    # è°ƒè¯•ç”¨ï¼šç›´æ¥æ‰“å°è¿™å›åˆçš„æŒ‡æ ‡
    print(
        f"[Episode {episode_idx}] avg_jerk = {avg_jerk:.4f}, "
        f"avg|sCTE| = {avg_scte:.4f}, signed_mean_sCTE = {signed_mean_scte:.4f}, "
        f"net_energy = {episode_energy_net_kwh:.4f} kWh"
    )


    # --- 4. ç´¯ç§¯ç»Ÿè®¡ ---
    eval_stats["rewards"].append(episode_reward)
    eval_stats["costs"].append(episode_cost)
    eval_stats["lengths"].append(episode_len)
    eval_stats["discounted_costs"].append(episode_discounted_cost)
    eval_stats["avg_jerk"].append(avg_jerk)
    eval_stats["avg_scte"].append(avg_scte)
    eval_stats["episode_energy_net_kwh"].append(episode_energy_net_kwh)
    eval_stats["episode_energy_consume_kwh"].append(episode_energy_consume_kwh)
    eval_stats["episode_energy_regen_kwh"].append(episode_energy_regen_kwh)

    # --- 5. ç»˜å›¾ä¿å­˜ ---
    plot_filename_base = f"ep_{episode_idx}_{outcome}"
    plot_base_path = os.path.join(log_save_dir, plot_filename_base)

    # 5.1 RL Agent é€Ÿåº¦æ›²çº¿ + å¥–åŠ±å†å²
    if hasattr(env, "rl_agent") and getattr(env, "rl_agent", None) is not None:
        try:
            speed_curve = env.rl_agent.get_speed_history()
            if speed_curve:
                plt.figure(figsize=(12, 6))
                plt.plot(speed_curve, linewidth=2.5, label="RL Agent")
                plt.title(f"RL Agent Speed Curve (Episode {episode_idx})")
                plt.xlabel("Time Step")
                plt.ylabel("Speed (m/s)")
                plt.grid(True)
                plt.xlim(0, max(1, len(speed_curve)))
                plt.ylim(0, 15)
                plt.legend()
                plt.tight_layout()
                rl_plot_path = os.path.join(
                    log_save_dir, f"{plot_filename_base}_rl_speed.png"
                )
                plt.savefig(rl_plot_path)
                plt.close()
                print(f"RL Agent é€Ÿåº¦å›¾å·²ä¿å­˜åˆ°: {rl_plot_path}")

            # RL Agent å¥–åŠ±å†å²å›¾ï¼ˆè¯¥æ–¹æ³•å†…éƒ¨åº”è‡ªè¡Œä¿å­˜ / å…³é—­ï¼‰
            env.rl_agent.plot_reward_history(save_path_base=plot_base_path)

        except Exception as e:
            print(f"ä¿å­˜ RL Agent é€Ÿåº¦/å¥–åŠ±å›¾å¤±è´¥: {e}")

    # 5.2 NPC è½¦è¾†é€Ÿåº¦æ›²çº¿
    try:
        all_npc_data = []
        if hasattr(env, "traffic_manager"):
            tm = env.traffic_manager
            if getattr(tm, "completed_vehicles_data", None):
                all_npc_data.extend(tm.completed_vehicles_data)
            for vehicle in getattr(tm, "vehicles", []):
                if not getattr(vehicle, "is_rl_agent", False):
                    all_npc_data.append(
                        {
                            "id": vehicle.vehicle_id,
                            "history": vehicle.get_speed_history(),
                        }
                    )

        if any(data["history"] for data in all_npc_data):
            plt.figure(figsize=(12, 6))
            max_len_npc = 0
            for vehicle_data in all_npc_data:
                if vehicle_data["history"]:
                    plt.plot(
                        vehicle_data["history"],
                        label=f"NPC #{vehicle_data['id']}",
                    )
                    max_len_npc = max(max_len_npc, len(vehicle_data["history"]))

            plt.title(f"NPC Vehicles Speed Curves (Episode {episode_idx})")
            plt.xlabel("Time Step")
            plt.ylabel("Speed (m/s)")
            plt.grid(True)
            plt.xlim(0, max(1, max_len_npc))
            plt.ylim(0, 15)
            plt.legend(loc="best", fontsize="small")
            plt.tight_layout()
            npc_plot_path = os.path.join(
                log_save_dir, f"{plot_filename_base}_npc_speed.png"
            )
            plt.savefig(npc_plot_path)
            plt.close()
            print(f"NPC è½¦è¾†é€Ÿåº¦å›¾å·²ä¿å­˜åˆ°: {npc_plot_path}")
    except Exception as e:
        print(f"ä¿å­˜ NPC è½¦è¾†é€Ÿåº¦å›¾å¤±è´¥: {e}")

    return outcome


def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„æ™ºèƒ½ä½“è¯„ä¼°æµç¨‹ï¼ˆæ—  GUIï¼‰
    """
    args = parse_args()

    # --- è®¾ç½®éšæœºç§å­ ---
    set_seed(args.seed)
    print(f"--- Setting random seed to {args.seed} ---")

    GAMMA = 0.99  # å‡è®¾ä¸è®­ç»ƒæ—¶ç›¸åŒ

    # --- 2. åˆ›å»ºç¯å¢ƒï¼ˆæ—  GameEngineï¼‰ ---
    env = TrafficEnv(scenario=args.scenario)
    env.reset(seed=args.seed)

    if hasattr(env, "traffic_manager") and hasattr(
        env.traffic_manager, "set_hv_speed_scaling"
    ):
        env.traffic_manager.set_hv_speed_scaling(1.0)
    else:
        print(
            "è­¦å‘Šï¼šæ— æ³•åœ¨TrafficManagerä¸­æ‰¾åˆ°set_hv_speed_scalingæ–¹æ³•ã€‚HVé€Ÿåº¦å¯èƒ½ä¸æ­£ç¡®ã€‚"
        )

    # --- 3. åŠ è½½æ¨¡å‹ ---
    print(f"--- Loading {args.algo.upper()} Agent from {args.model_path} ---")
    try:
        if args.algo.startswith("ppo_lagrangian"):
            # è‡ªå®šä¹‰çš„ Lagrangian PPO
            model = PPOLagrangian.load(args.model_path, env=env)
        elif args.algo.startswith("ppo"):
            # æ™®é€š SB3-PPO
            model = PPO.load(args.model_path, env=env)
        elif args.algo.startswith("sagi_ppo"):
            # SAGI-PPO è‡ªå®šä¹‰ç±»
            model = SAGIPPO.load(args.model_path, env=env)
        else:
            raise ValueError(f"Unknown algorithm for loading: {args.algo}")
    except Exception as e:
        print(f"Error loading model: {e}")
        print("è¯·ç¡®ä¿æ‚¨æä¾›çš„è·¯å¾„æ˜¯ä¸€ä¸ªç”±Stable Baselines3ä¿å­˜çš„.zipæ¨¡å‹æ–‡ä»¶ï¼Œ")
        print("å¹¶ä¸”æ‚¨çš„agent.py, config.pyç­‰æ–‡ä»¶ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ã€‚")
        return

    # --- 4. æ—¥å¿—ç›®å½• ---
    model_name = os.path.splitext(os.path.basename(args.model_path))[0]
    log_save_dir = os.path.join(
        "evaluation_logs", f"{model_name}_{time.strftime('%Y%m%d-%H%M%S')}"
    )
    os.makedirs(log_save_dir, exist_ok=True)
    print(f"è¯„ä¼°æ—¥å¿—å°†ä¿å­˜åœ¨: {log_save_dir}")

    # è®°å½•è¯„ä¼°é…ç½®
    eval_config_path = os.path.join(log_save_dir, "eval_config.txt")
    with open(eval_config_path, "w") as f:
        f.write(f"Evaluation date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Algorithm: {args.algo}\n")
        f.write(f"Model Path: {args.model_path}\n")
        f.write(f"Episodes: {args.num_episodes}\n")
        f.write(f"Seed: {args.seed}\n")
        f.write(f"Scenario: {args.scenario}\n")
    print(f"Evaluation configuration saved to {eval_config_path}")

    # --- 5. è¯„ä¼°ç»Ÿè®¡å®¹å™¨ ---
    eval_stats = {
        "rewards": [],
        "costs": [],
        "lengths": [],
        "discounted_costs": [],
        "avg_jerk": [],
        "avg_scte": [],
        "success": 0,
        "collision": 0,
        "timeout": 0,
        "off_track": 0,
        "episode_energy_net_kwh": [],      # å‡€ç”µèƒ½ = æ¶ˆè€— - å›æ”¶
        "episode_energy_consume_kwh": [],  # åªç®—æ¶ˆè€—
        "episode_energy_regen_kwh": [],    # å›æ”¶çš„èƒ½é‡
    }

    print(
        f"--- å¼€å§‹æ—  GUI è¯„ä¼°: å…± {args.num_episodes} ä¸ªå›åˆ, åœºæ™¯ = {args.scenario} ---"
    )

    # --- 6. ä¸»è¯„ä¼°å¾ªç¯ï¼ˆçº¯ç¯å¢ƒäº¤äº’ï¼‰ ---
    try:
        for ep in range(1, args.num_episodes + 1):
            reset_options = {
                "scenario": args.scenario,
                "algo": args.algo,
            }
            state, info = env.reset(
                seed=args.seed + ep,
                options=reset_options,
            )

            episode_reward = 0.0
            episode_cost = 0.0
            episode_len = 0
            episode_discounted_cost = 0.0

            print(f"\n--- [Episode {ep}/{args.num_episodes}] ---")

            while True:
                action, _states = model.predict(state, deterministic=True)
                next_state, reward, terminated, truncated, info = env.step(action)
                state = next_state

                step_cost = info.get("cost", 0.0)
                episode_reward += reward
                episode_cost += step_cost
                episode_len += 1
                episode_discounted_cost += (GAMMA ** (episode_len - 1)) * step_cost

                done = terminated or truncated
                if done:
                    handle_episode_end(
                        env=env,
                        info=info,
                        eval_stats=eval_stats,
                        episode_reward=episode_reward,
                        episode_cost=episode_cost,
                        episode_len=episode_len,
                        episode_discounted_cost=episode_discounted_cost,
                        episode_idx=ep,
                        log_save_dir=log_save_dir,
                        truncated=truncated,
                    )
                    break

    except KeyboardInterrupt:
        print("\nè¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")

    finally:
        print("\n--- æ­£åœ¨ä¿å­˜åœºæ™¯ä¸­å‰©ä½™è½¦è¾†çš„æ•°æ® (Unfinished Vehicles) ---")
        if hasattr(env, "traffic_manager"):
            # è¿™ä¼šè°ƒç”¨ vehicle.save_trajectory_to_csvï¼Œæ–‡ä»¶åä¸º trajectory_unfinished_X.csv
            # è¿™äº›æ–‡ä»¶ä¼šä¿å­˜åœ¨ evaluation_logs ç›®å½•ä¸‹ (å› ä¸º vehicle.py ä¸­æœ‰æ£€æµ‹é€»è¾‘)
            env.traffic_manager.clear_all_vehicles()

    # --- 7. æ‰“å°æœ€ç»ˆé‡åŒ–è¯„ä¼°ç»“æœ ---
    print("\n\n" + "=" * 30)
    print("--- è¯„ä¼°ç»“æœæ±‡æ€» ---")
    print("=" * 30)

    num_episodes_finished = len(eval_stats["rewards"])
    if num_episodes_finished > 0:
        avg_travel_time = np.mean(eval_stats["lengths"]) * env.dt
        print(f"å®Œæˆå›åˆæ•°: {num_episodes_finished}")
        print(
            f"å¹³å‡å¥–åŠ±: {np.mean(eval_stats['rewards']):.2f} Â± {np.std(eval_stats['rewards']):.2f}"
        )
        print(
            f"å¹³å‡å›åˆæˆæœ¬: {np.mean(eval_stats['costs']):.2f} Â± {np.std(eval_stats['costs']):.2f}"
        )
        print(f"å¹³å‡é€šè¡Œæ—¶é—´ (ç§’): {avg_travel_time:.2f}")
        print(
            f"å¹³å‡åŠ åŠ é€Ÿåº¦ (m/s^3): {np.mean(eval_stats.get('avg_jerk', [0])):.4f}"
        )
        print(
            f"å¹³å‡ |sCTE| (ç±³): {np.mean(eval_stats.get('avg_scte', [0])):.4f}"
        )
        # ğŸ”¥ å¹³å‡èƒ½è€—ç›¸å…³æŒ‡æ ‡
        if eval_stats.get("episode_energy_net_kwh"):
            mean_net = np.mean(eval_stats["episode_energy_net_kwh"])
            mean_consume = np.mean(eval_stats["episode_energy_consume_kwh"])
            mean_regen = np.mean(eval_stats["episode_energy_regen_kwh"])
            print(
                f"å¹³å‡æ¯å›åˆå‡€ç”µèƒ½: {mean_net:.4f} kWh "
                f"(æ¶ˆè€— {mean_consume:.4f} kWh, å›æ”¶ {mean_regen:.4f} kWh)"
            )

        print("-" * 20)
        print(f"æˆåŠŸç‡: {eval_stats['success'] / num_episodes_finished:.2%}")
        print(f"ç¢°æ’ç‡: {eval_stats['collision'] / num_episodes_finished:.2%}")
        print(f"åç¦»è½¨è¿¹ç‡: {eval_stats['off_track'] / num_episodes_finished:.2%}")
        print(f"è¶…æ—¶ç‡: {eval_stats['timeout'] / num_episodes_finished:.2%}")
    else:
        print("æ²¡æœ‰å®Œæˆä»»ä½•å›åˆçš„è¯„ä¼°ã€‚")


if __name__ == "__main__":
    main()
