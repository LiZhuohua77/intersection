import warnings
# [å…³é”®ä¿®æ­£] ä»Ž typing æ¨¡å—å¯¼å…¥ Generator å’Œ Tuple
from typing import Any, Dict, Optional, Type, Union, Generator, Tuple

import numpy as np
import torch
from gymnasium import spaces
from torch.nn import functional as F
import torch.nn as nn
import torch as th

from stable_baselines3 import PPO
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.buffers import RolloutBuffer, RolloutBufferSamples
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.utils import obs_as_tensor
from stable_baselines3.common.torch_layers import create_mlp
from stable_baselines3.common.vec_env import VecEnv, VecNormalize


# ==============================================================================
# 1. è‡ªå®šä¹‰ç­–ç•¥ç½‘ç»œ (æ­¤éƒ¨åˆ†å·²ç¨³å®šï¼Œæ— éœ€ä¿®æ”¹)
# ==============================================================================
class ActorCriticCostPolicy(ActorCriticPolicy):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        cost_vf_arch = self.net_arch.get('cost_vf', [])
        if cost_vf_arch:
            self.cost_value_net = nn.Sequential(*create_mlp(
                self.mlp_extractor.latent_dim_vf, 1, net_arch=cost_vf_arch, activation_fn=self.activation_fn
            ))
        else:
            self.cost_value_net = nn.Identity()

    def evaluate_actions(self, obs: torch.Tensor, actions: torch.Tensor):
        """
        [FINAL CORRECTED VERSION]
        This version is based on your provided source code and removes the problematic SDE check.
        """
        # Get latent features using the standard, correct two-step process
        features = self.extract_features(obs)
        latent_pi, latent_vf = self.mlp_extractor(features)

        # [FIXED] The entire block checking for 'sde_features_extractor' has been removed.
        # We pass latent_pi directly to the distribution network.
        distribution = self._get_action_dist_from_latent(latent_pi)

        log_prob = distribution.log_prob(actions)

        # Calculate reward and cost values
        values = self.value_net(latent_vf)
        cost_values = self.cost_value_net(latent_vf)

        entropy = distribution.entropy()

        return values, cost_values, log_prob, entropy


# ==============================================================================
# 2. è‡ªå®šä¹‰ç»éªŒç¼“å†²åŒº (å¯¹ get æ–¹æ³•çš„ç±»åž‹æ ‡æ³¨è¿›è¡Œä¿®æ­£)
# ==============================================================================
class SAGIRolloutBuffer(RolloutBuffer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.costs = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_values = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_advantages = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_returns = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)


    def reset(self) -> None:
        super().reset()
        self.costs = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_values = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_advantages = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.cost_returns = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)

    def add(self, cost: np.ndarray, cost_value: torch.Tensor, **kwargs) -> None:
        self.costs[self.pos] = np.array(cost).copy()
        self.cost_values[self.pos] = cost_value.clone().cpu().numpy().flatten()
        super().add(**kwargs)

    def compute_returns_and_advantage(self, last_values: torch.Tensor, last_cost_values: torch.Tensor, dones: np.ndarray):
        super().compute_returns_and_advantage(last_values, dones)
        last_cost_values = last_cost_values.clone().cpu().numpy().flatten()
        last_gae_lam = 0
        for step in reversed(range(self.buffer_size)):
            if step == self.buffer_size - 1:
                next_non_terminal = 1.0 - dones
                next_cost_values = last_cost_values
            else:
                next_non_terminal = 1.0 - self.episode_starts[step + 1]
                next_cost_values = self.cost_values[step + 1]
            delta = self.costs[step] + self.gamma * next_cost_values * next_non_terminal - self.cost_values[step]
            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam
            self.cost_advantages[step] = last_gae_lam
        self.cost_returns = self.cost_advantages + self.cost_values

    def get_mean_episode_costs(self) -> float:
            """
            [CORRECTED VERSION]
            Computes the mean of discounted costs for all completed episodes in the buffer.
            """
            episode_costs = []
            # A running total of discounted costs for each environment
            current_discounted_costs = np.zeros(self.n_envs)
            
            # Iterate backwards through the buffer
            for step in reversed(range(self.buffer_size)):
                # Create a boolean mask for environments where a new episode starts at this step
                is_start_mask = self.episode_starts[step].astype(bool)

                # If any episodes start here, it means episodes have just ended.
                # Their total accumulated costs are in `current_discounted_costs`.
                if np.any(is_start_mask):
                    # Append the accumulated costs from the completed episodes to our list
                    episode_costs.extend(current_discounted_costs[is_start_mask])
                    # Reset the accumulator for those environments
                    current_discounted_costs[is_start_mask] = 0.0

                # Accumulate the cost for the current step into the running total.
                # This happens *after* we've potentially recorded and reset.
                current_discounted_costs = self.costs[step] + self.gamma * current_discounted_costs
            
            # The loop misses the very first set of episodes that don't start with a 'True'
            # in the buffer, but this is standard and acceptable. We only average full episodes.
            
            if not episode_costs:
                warnings.warn(
                    "No full episodes found in the rollout buffer. "
                    "Consider increasing n_rollout_steps or Rewarding faster termination."
                )
                return 0.0
            
            return np.mean(episode_costs)

# ==============================================================================
# 3. SAGI-PPO ç®—æ³• (æ­¤éƒ¨åˆ†å·²ç¨³å®šï¼Œæ— éœ€ä¿®æ”¹)
# ==============================================================================
class SAGIPPO(PPO):
    policy_aliases: Dict[str, Type[ActorCriticPolicy]] = { "MlpPolicy": ActorCriticCostPolicy }

    def __init__(self, policy, env, cost_limit: float = 25.0, lambda_lr: float = 0.035, cost_vf_coef: float = 0.5, **kwargs):
        self.cost_limit = cost_limit
        self.lambda_lr = lambda_lr
        self.cost_vf_coef = cost_vf_coef
        self.lambda_ = 0.0
        super().__init__(policy=policy, env=env, rollout_buffer_class=SAGIRolloutBuffer, **kwargs)

    def train(self) -> None:
        """
        [æœ€ç»ˆä¿®æ­£]
        è°ƒæ•´äº†ä»£ç çš„æ‰§è¡Œé¡ºåºï¼Œä»¥é€‚åº” get() æ–¹æ³•çš„æƒ°æ€§æ±‚å€¼ç‰¹æ€§ï¼Œ
        ä»Žæ ¹æºä¸Šè§£å†³åå¤å‡ºçŽ°çš„å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜ã€‚
        """
        self.policy.set_training_mode(True)

        # ======================= åœ¨è¿™é‡Œæ’å…¥æ·±åº¦è¯Šæ–­ä»£ç  =======================
        total_costs_in_buffer = np.sum(self.rollout_buffer.costs)
        total_episode_starts_in_buffer = np.sum(self.rollout_buffer.episode_starts)
        
        print("\n" + "="*50)
        print("ðŸ” Deep Dive Buffer Inspection")
        print(f"    Buffer Shape (Steps, Envs): {self.rollout_buffer.costs.shape}")
        print(f"    Total costs in buffer: {total_costs_in_buffer}")
        print(f"    Total episode starts in buffer: {total_episode_starts_in_buffer}")
        
        # æ‰“å°å‰5ä¸ªæ—¶é—´æ­¥çš„ cost å’Œ episode_start æ•°æ®ï¼Œä»¥ä¾›æŠ½æŸ¥
        print("    Sample Data (first 5 steps):")
        for i in range(min(5, self.rollout_buffer.buffer_size)):
            print(f"      Step {i}: costs={self.rollout_buffer.costs[i]}, starts={self.rollout_buffer.episode_starts[i]}")
        print("="*50 + "\n")
        # ======================= è¯Šæ–­ä»£ç ç»“æŸ =======================

        self._update_learning_rate(self.policy.optimizer)
        clip_range = self.clip_range(self._current_progress_remaining)

        j_c_k = self.rollout_buffer.get_mean_episode_costs()
        c = j_c_k - self.cost_limit

        # ==================== å…³é”®ä¿®æ­£ï¼šè°ƒæ•´æ‰§è¡Œé¡ºåº ====================
        # 1. é¦–å…ˆï¼ŒèŽ·å–ä¸€æ¬¡å®Œæ•´çš„æ‰¹æ¬¡æ•°æ®ã€‚
        #    è¿™ä¸€æ­¥ä¼šè§¦å‘ get() å†…éƒ¨çš„ flatten é€»è¾‘ï¼Œå°† self.advantages ç­‰æ•°ç»„çš„å½¢çŠ¶å˜ä¸º (4096, 1)
        full_batch_generator = self.rollout_buffer.get(batch_size=self.rollout_buffer.buffer_size * self.n_envs)
        full_batch = next(full_batch_generator)

        # 2. çŽ°åœ¨ï¼Œself.rollout_buffer.advantages å·²ç»æ˜¯è¢« flatten åŽçš„æ­£ç¡®å½¢çŠ¶äº†ï¼Œæˆ‘ä»¬å†å¤åˆ¶å®ƒã€‚
        original_advantages = self.rollout_buffer.advantages.copy()

        # 3. æ‰‹åŠ¨ flatten æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°ç»„ï¼Œä½¿å…¶å½¢çŠ¶ä¸Ž original_advantages ä¿æŒå®Œå…¨ä¸€è‡´ã€‚
        cost_advantages_flat = self.rollout_buffer.cost_advantages.swapaxes(0, 1).reshape(-1, 1)
        cost_returns_flat = self.rollout_buffer.cost_returns.swapaxes(0, 1).reshape(-1, 1)
        
        # 4. çŽ°åœ¨ä¸¤ä¸ªæ•°ç»„çš„å½¢çŠ¶éƒ½æ˜¯ (4096, 1)ï¼Œå¯ä»¥å®‰å…¨åœ°è¿›è¡Œè¿ç®—ã€‚
        cost_adv_for_update = cost_advantages_flat
        # ===============================================================

        # --- è®¡ç®—æ¢¯åº¦å†…ç§¯ p ---
        # æˆ‘ä»¬å·²ç»èŽ·å–äº† full_batchï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨
        reward_advantages_p = torch.as_tensor(original_advantages, device=self.device).flatten()
        cost_advantages_p = torch.as_tensor(cost_adv_for_update, device=self.device).flatten()
        
        self.policy.train()
        _, _, log_prob, _ = self.policy.evaluate_actions(full_batch.observations, full_batch.actions)
        ratio = torch.exp(log_prob - full_batch.old_log_prob)
        
        reward_surrogate_loss = -torch.min(reward_advantages_p * ratio, reward_advantages_p * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)).mean()
        cost_surrogate_loss = -torch.min(cost_advantages_p * ratio, cost_advantages_p * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)).mean()
        
        g_r_tensors = torch.autograd.grad(reward_surrogate_loss, self.policy.parameters(), retain_graph=True, allow_unused=True)
        g_r_flat = torch.cat([grad.flatten() for grad in g_r_tensors if grad is not None])
        g_c_tensors = torch.autograd.grad(cost_surrogate_loss, self.policy.parameters(), retain_graph=False, allow_unused=True)
        g_c_flat = torch.cat([grad.flatten() for grad in g_c_tensors if grad is not None])
        p = torch.dot(g_r_flat, g_c_flat).item() if g_r_flat.numel() > 0 and g_c_flat.numel() > 0 else 0.0
        
        self.logger.record("sagi/cost_surplus_c", c)
        self.logger.record("sagi/grad_inner_product_p", p)
        self.logger.record("sagi/lambda", self.lambda_)

        if c < 0 and p <= 0:
            self.logger.record("sagi/mode", "A")
            # æ¨¡å¼A: ä¿æŒåŽŸå§‹å¥–åŠ±ä¼˜åŠ¿ï¼Œå³ self.rollout_buffer.advantages å·²ç»æ˜¯ original_advantages
            self.rollout_buffer.advantages = original_advantages
        elif c > 0:
            self.logger.record("sagi/mode", "C")
            self.rollout_buffer.advantages = -cost_adv_for_update.copy()
        else:
            self.logger.record("sagi/mode", "B")
            self.rollout_buffer.advantages = (original_advantages - self.lambda_ * cost_adv_for_update) / (1 + self.lambda_)

        self.lambda_ = max(0, self.lambda_ + self.lambda_lr * c)

        # --- PPO å°æ‰¹æ¬¡æ›´æ–°å¾ªçŽ¯ ---
        # å†æ¬¡è°ƒç”¨ get() æ—¶ï¼Œç”±äºŽ generator_ready=Trueï¼Œå®ƒä¼šç›´æŽ¥ä»Žå·² flatten çš„æ•°æ®ä¸­åˆ›å»ºå°æ‰¹æ¬¡
        current_batch_start_idx = 0
        for rollout_data in self.rollout_buffer.get(self.batch_size):
            reward_values, cost_values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, rollout_data.actions)
            advantages = rollout_data.advantages
            if self.normalize_advantage:
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            ratio = torch.exp(log_prob - rollout_data.old_log_prob)
            policy_loss = -torch.min(advantages * ratio, advantages * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)).mean()
            
            batch_size = rollout_data.observations.shape[0]
            # æ‰‹åŠ¨åˆ‡ç‰‡èŽ·å–å¯¹åº”çš„å°æ‰¹æ¬¡æˆæœ¬å›žæŠ¥
            cost_returns_batch = cost_returns_flat[current_batch_start_idx : current_batch_start_idx + batch_size]
            current_batch_start_idx += batch_size

            value_loss = F.mse_loss(rollout_data.returns, reward_values.flatten())
            cost_value_loss = F.mse_loss(torch.as_tensor(cost_returns_batch, device=self.device).flatten(), cost_values.flatten())
            
            entropy_loss = -torch.mean(entropy) if entropy is not None else 0.0
            loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss + self.cost_vf_coef * cost_value_loss

            self.policy.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
            self.policy.optimizer.step()

        self._n_updates += 1
        # åœ¨è®­ç»ƒç»“æŸåŽæ¢å¤åŽŸå§‹çš„ advantagesï¼Œä»¥ä¾¿æ—¥å¿—è®°å½•ç­‰åŽç»­æ­¥éª¤ä½¿ç”¨
        self.rollout_buffer.advantages = original_advantages
    
    def collect_rollouts(self, env: VecEnv, callback: BaseCallback, rollout_buffer: SAGIRolloutBuffer, n_rollout_steps: int) -> bool:
        assert self._last_obs is not None
        self.policy.set_training_mode(False)
        n_steps = 0
        rollout_buffer.reset()
        callback.on_rollout_start()
        while n_steps < n_rollout_steps:
            with torch.no_grad():
                obs_tensor = obs_as_tensor(self._last_obs, self.device)
                features = self.policy.extract_features(obs_tensor)
                latent_pi, latent_vf = self.policy.mlp_extractor(features)
                distribution = self.policy._get_action_dist_from_latent(latent_pi)
                actions = distribution.get_actions(deterministic=False)
                log_probs = distribution.log_prob(actions)
                values = self.policy.value_net(latent_vf)
                cost_values = self.policy.cost_value_net(latent_vf)

            actions = actions.cpu().numpy()
            clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)
            new_obs, rewards, dones, infos = env.step(clipped_actions)
            costs = np.array([info.get("cost", 0) for info in infos])
            self.num_timesteps += env.num_envs
            if callback.on_step() is False: return False
            self._update_info_buffer(infos, dones)
            n_steps += 1
            if isinstance(self.action_space, spaces.Discrete): actions = actions.reshape(-1, 1)
            rollout_buffer.add(obs=self._last_obs, action=actions, reward=rewards, cost=costs, episode_start=self._last_episode_starts, value=values, cost_value=cost_values, log_prob=log_probs)
            self._last_obs = new_obs
            self._last_episode_starts = dones
        with torch.no_grad():
            last_obs_tensor = obs_as_tensor(new_obs, self.device)
            features = self.policy.extract_features(last_obs_tensor)
            _, latent_vf = self.policy.mlp_extractor(features)
            last_values = self.policy.value_net(latent_vf)
            last_cost_values = self.policy.cost_value_net(latent_vf)
        rollout_buffer.compute_returns_and_advantage(last_values=last_values, last_cost_values=last_cost_values, dones=dones)
        callback.on_rollout_end()
        return True